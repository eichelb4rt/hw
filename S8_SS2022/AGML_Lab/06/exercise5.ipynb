{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "683c5aac",
   "metadata": {},
   "source": [
    "# Exercise 5 - Image classification with Feature Maps (30 Points)\n",
    "\n",
    "In this exercise you will learn how feature maps can be used improve the accuracy of image classifiers based on softmax regression.\n",
    "\n",
    "In the event of a persistent problem, do not hesitate to contact the course instructors under\n",
    "- christoph.staudt@uni-jena.de\n",
    "\n",
    "### Submission\n",
    "\n",
    "- Deadline of submission:\n",
    "        26.05.2021 23:59\n",
    "- Submission on [moodle page](https://moodle.uni-jena.de/course/view.php?id=28746)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdd86d1f",
   "metadata": {},
   "source": [
    "### Installing all required packages\n",
    "\n",
    "You will need pytorch for this exercise (see more at the end), there are often issues with existing packages and interdependencies. If you use conda you can use the following command to create a completely new environment with all required packages that should work quite well:\n",
    "`conda create -n torch -c pytorch -c conda-forge scikit-learn numpy matplotlib jupyter pandas pytorch torchvision tqdm`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e7c57a5",
   "metadata": {},
   "source": [
    "## Softmax Regression\n",
    "\n",
    "In this classification task we have more than two classes, therefore we use *softmax regression* as the natural extension of logistic regression (see exercise 2).\n",
    "\n",
    "Recall, that in softmax regression we have features $x\\in\\mathbb{R}^n$ which we want to classify into $k$ classes.\n",
    "The goal is then to find a matrix $\\theta\\in\\mathbb{R}^{n\\times k}$, so that\n",
    "\n",
    "\\begin{equation}\n",
    "p(y=i|x) = \\cfrac{\\exp\\left(x^T\\theta^{(i)}\\right)}{\\sum_{j=1}^{k}\\exp\\left(x^T\\theta^{(j)}\\right)}\n",
    "\\end{equation}\n",
    "\n",
    "where $\\theta^{(i)}$ is the i-th column of $\\theta$.\n",
    "\n",
    "## Dataset\n",
    "\n",
    "For this exercise we use a preprocessed subset of a dataset for [Scene Classification](https://www.kaggle.com/nitishabharathi/scene-classification). All images are 128x128 grayscale images from one of three classes:\n",
    "- Buildings\n",
    "- Forest\n",
    "- Mountains\n",
    "\n",
    "The goal of this exercise is to train classifiers that can classify images into their respective scenery.\n",
    "\n",
    "### Task 1 (1 Point)\n",
    "\n",
    "The dataset is stored as numpy arrays under `X.npy` (images) and `Y.npy` (labels).\n",
    "\n",
    "Load the dataset and display an image per class using matplotlib. You need to reshape X to the right dimensions again, to show the proper image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63e441c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.load('X.npy')\n",
    "Y = np.load('Y.npy')\n",
    "\n",
    "# TODO display images"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e062e75b",
   "metadata": {},
   "source": [
    "# No feature map\n",
    "\n",
    "A first approach for Softmax Regression would be to take the raw pixels as features.\n",
    "\n",
    "### Task 2 (3 Points)\n",
    "\n",
    "In scikit-learn, softmax regression is implemented within the [logistic regression](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html) class. \n",
    "\n",
    "Use scikit learn to fit softmax regression on the images and evaluate the accuracy on train (75%)- and test (25%) data.\n",
    "What do you observe?\n",
    "\n",
    "Hint: \n",
    "- Use the keyword \"multinomial\"\n",
    "- Ignore the max_it warning and that it didn't converge you will see the accuracy on the training set is already pretty high and we cannot expect the accuracy on the test set to get much higher. This is also true for the remaining Tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a86f674b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.25, random_state=0)\n",
    "\n",
    "# TODO: fit softmax regression\n",
    "\n",
    "# TODO: get accuracy on train- and test data\n",
    "\n",
    "assert np.isclose(train_acc,0.98, 0.05)\n",
    "assert np.isclose(test_acc,0.43, 0.05)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62f3440c",
   "metadata": {},
   "source": [
    "## Normalization\n",
    "\n",
    "You probably observed, that softmax regression on the raw pixel values does not lead to good results. While the training accuracy is quite high, the test accuracy is very low.\n",
    "\n",
    "One reason for this is that pixel values are *unnormalized*. This means, different dimensions of the features are in different ranges and have different statistical properties.\n",
    "\n",
    "When we use a regressor, we heavily rely on the dot product between two vectors as distance measure.\n",
    "\n",
    "Recall, that for two vectors $x^{(1)}, x^{(2)}\\in\\mathbb{R}^d$ the dot product is defined as\n",
    "\n",
    "\\begin{align}\n",
    "(x^{(1)})^Tx^{(2)} = \\sum_{i=1}^d x^{(1)}_ix^{(2)}_i\n",
    "\\end{align}\n",
    "\n",
    "In this distance measure, products over different feature dimensions are added together. If we dont want certain dimensions to dominate others, we have to normalize the features.\n",
    "\n",
    "### Task 3 (2 Points)\n",
    "\n",
    "Visualize the mean values and the variance for each feature. What do you observe?\n",
    "\n",
    "Hint:\n",
    "- Reshape the values into image dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f98918c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Visualize mean and variance of pixel features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94da0bc6",
   "metadata": {},
   "source": [
    "### Task 4 (2 Points)\n",
    "\n",
    "Normalize the dataset, so that each feature dimension has mean 0 and standard deviation 1.\n",
    "Similar to task 4, visualize the mean and variance values for the features of the normalized data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d7ee39e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: normalize images\n",
    "\n",
    "# TODO: visualize mean and variance of normalized pixel features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fb489a7",
   "metadata": {},
   "source": [
    "### Task 5 (1 Point)\n",
    "Now use logistic regression on the transformed features. Calculate the accuracies on the train- and testset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6acc6f7d",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# TODO: use logistic regression on normalized features\n",
    "assert np.isclose(train_acc,0.999, 0.05)\n",
    "assert np.isclose(test_acc,0.523, 0.05)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60c60c3f",
   "metadata": {},
   "source": [
    "Altough we normalized the features, our accuracy is still quite low.\n",
    "\n",
    "## Task 6 (2 Points)\n",
    "\n",
    "Another common off the shelf technique is to perform dimensionality reduction using the [principal component analysis](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html). Write a function that takes trainings and test data as well as a number of components to take from the PCA. It should fit the PCA (use the class linked above) on the trainings data, then transform the the trainigns and test data, fit the logistic regression on the transformed trainings data and finally return the accuracy on the training and test data.\n",
    "\n",
    "Run the function for different numbers of components as given in the vairable `components`, print the resulting accuracies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0a20e27",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_pca(x_train, y_train, x_test, y_test,n_components=3):\n",
    "    # TODO: Fit x_train for n_components\n",
    "\n",
    "    # TODO: Transform x_train and x_test\n",
    "\n",
    "    # TODO: fit regression model on transformed x_train\n",
    "\n",
    "    # TODO: Calculate accuarcy\n",
    "\n",
    "components = [3,5,10,30,50]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f08e9d8",
   "metadata": {},
   "source": [
    "Using the PCA improved the performance quite a bit, but it is still not very good.\n",
    "\n",
    "This is because we take pixel values as features, which is extremely inconsistent. \n",
    "\n",
    "# Handcrafted features\n",
    "\n",
    "**Feature maps** attempt to bypass this inconsistency by transforming the pixel vector into a different, more consistent feature vector. \n",
    "\n",
    "Especially in the field of image classification, a lot of attempts were made to derive features from images trough a pipeline of preprocessing.\n",
    "\n",
    "We want to use edges in our images as features.\n",
    "Edges are visual boundaries of objects. Hence they contain information about objects in images.\n",
    "In grayscale images, we can interpret edges as sudden changes of pixel intensities in some direction.\n",
    "\n",
    "## Convolution\n",
    "In order to detect these changes, we will use linear filters on the images. A linear filter is determined by a kernel matrix $K$, that describes how a new pixel is created by a linear combination of its neighbouring pixels.\n",
    "\n",
    "As an example, consider the following [kernel](https://en.wikipedia.org/wiki/Kernel_(image_processing)) matrix:\n",
    "\n",
    "\\begin{align}\n",
    "K=\\begin{bmatrix}\n",
    "\\frac{1}{9}&\\frac{1}{9}&\\frac{1}{9}\\\\\n",
    "\\frac{1}{9}&\\frac{1}{9}&\\frac{1}{9}\\\\\n",
    "\\frac{1}{9}&\\frac{1}{9}&\\frac{1}{9}\n",
    "\\end{bmatrix}\\end{align}\n",
    "\n",
    "For the calculation of a new pixel, we take the $3\\times3$ neighbourhood, i.e. the 8 pixels around the original pixel and the pixel itself, multiply each of these pixels with its corresponding entry in $K$ and sum everything up. The example kernel will simply set a pixel to the average of its neighbouring pixels and is called *average filter*. We consider only kernels with *odd dimensions*.\n",
    "\n",
    "For the pixels at the border of the image, there is no full neighbourhood and we will assume those missing pixels to be 0. Numpy has the [pad](https://numpy.org/doc/stable/reference/generated/numpy.pad.html) function, that takes an array and `pad_width` and by default adds `pad_width` zeros on each side of the array. Since numpby 1.20 there is another useful function for this task: [sliding_window_view](https://numpy.org/devdocs/reference/generated/numpy.lib.stride_tricks.sliding_window_view.html) takes an array and a window size (the kernel size for us) and returns a view (thus it doesn't copy the data) that has two new dimensions with the same size as the given kernel size. It's first two dimensions are a bit smaller since it always takes a full window and does not use the border pixels. If you combine pad with the sliding window you can create an array that has the correct neighbourhood at i,j which is a matrix with the same size as the kernel:\n",
    "`sliding_window_view(padded_array,kernel.shape)[i,j] =  neighbourhood[i,j]`.\n",
    "\n",
    "Finally this can be combined with [einsum](https://numpy.org/doc/stable/reference/generated/numpy.einsum.html) to create a fast convolution. For an introduction you can have a look at this [blog post](https://ajcr.net/Basic-guide-to-einsum/). To get the right explicit subscripts string, you need to think about the following questions:\n",
    "1. How many dimensions / indices does each array have?\n",
    "2. Which of those should get the same name? Remember that indices can only have the same name if they have the same dimension. By this limitation alone its often clear which indices should be repeated on the left side of the '->'.\n",
    "3. How many dimensions should the result have and which of the indices should be in it? Remeber you will sum over all indices missing in the result. This will give you the right side of the '->'.\n",
    "\n",
    "If you can't get einsum to work, please reach out or just use several for loops as indicated in the linked wikipedia article. That will be quite slow however.\n",
    "\n",
    "### Task 6 (4 Points)\n",
    "Implement a function, that performs convolution on an image with a given kernel. Consider neighbourhood values that are not within the image as zero (zero padding). Filter the third image from the dataset with the average filter from the example and display the result using matplotlib."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94221d8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convolute(img, kernel):\n",
    "    # TODO: Implement convolution function\n",
    "    pass\n",
    "\n",
    "# TODO: perform average filtering on the third image\n",
    "img = X[2].reshape((128,128))\n",
    "kernel = # 3x3 average kernel from above\n",
    "res = convolute(img,kernel)\n",
    "assert np.isclose(res[0,0],78.44444444,0.001)\n",
    "assert np.isclose(res[1,1],175,0.001)\n",
    "# Plot res"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e04248a",
   "metadata": {},
   "source": [
    "As mentioned before, edges correspond to a sudden change of the pixel intensities. \n",
    "We can detect these changes in horizontal and vertical direction with the kernel matrices\n",
    "\n",
    "\\begin{align*}\n",
    "K_x = \\begin{bmatrix}\n",
    "-1&0&1\\\\\n",
    "-2&0&2\\\\\n",
    "-1&0&1\n",
    "\\end{bmatrix}\\end{align*} \n",
    "\\begin{align*}K_y = \\begin{bmatrix}\n",
    "-1&-2&-1\\\\\n",
    "0&0&0\\\\\n",
    "1&2&1\n",
    "\\end{bmatrix}\\end{align*}\n",
    "\n",
    "These kernel matrices are called *Sobel Filters* and serve as an approximation to the gradient of the pixel values.\n",
    "Let $I_x$ and $I_y$ be the images obtained by filtering our original image with $K_x$ and $K_y$ respectively.\n",
    "\n",
    "The magnitude of our gradient in both directions can be calculated by \n",
    "\n",
    "\\begin{align*}\n",
    "|G| = \\sqrt{I_x^2+I_y^2}\n",
    "\\end{align*}\n",
    "\n",
    "\n",
    "### Task 7 (3 Points)\n",
    "\n",
    "Implement a function that calculates the gradient image for a given image. Test this function with the third image and visualize it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06e4033a",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def grad_img(img):\n",
    "    # TODO: Calculate gradient image (2 Points)\n",
    "    pass\n",
    "    \n",
    "# TODO: calculate + display gradient image\n",
    "img = X[2].reshape((128,128))\n",
    "grad = grad_img(img)\n",
    "# TODO: Plot grad\n",
    "\n",
    "assert np.isclose(grad[0,0], 745.3026231,0.001)\n",
    "assert np.isclose(grad[1,1], 21.21320344,0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cb60881",
   "metadata": {},
   "source": [
    "### Task 8 (2 Points)\n",
    "\n",
    "Now we want to use this function to create a transformed dataset. Implement a function that takes a dataset of images and transforms it into a dataset of gradient images. Apply this function to the train- and testset.\n",
    "\n",
    "Note: This might take a while. Have a look at the python package [tqdm](https://tqdm.github.io/) to visualize the conversion process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dda6220",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_grad(X):\n",
    "    # TODO: transform dataset into gradient image vectors\n",
    "    pass\n",
    "\n",
    "# TODO: transform train- and testset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "532a1cca",
   "metadata": {},
   "source": [
    "### Task 9 (1 Point)\n",
    "Now use logistic regression on the transformed trainset. What are the accuracies on the train- and testset (Print them!)? Use your function for PCA from above with the different numbers of components on the transformed datasets as well. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0922c6f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: apply softmax regression on transformed train- and testset\n",
    "\n",
    "# TODO: Apply function for pca with various component sizes again"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c95342e6",
   "metadata": {},
   "source": [
    "# Deep Learning Features\n",
    "\n",
    "In practice, Deep Learning features are the state of the art for most image classification tasks.\n",
    "\n",
    "Deep Learning features are obtained by processing an input feature with a trained *deep neural network*.\n",
    "Such a neural network consists of multiple processing steps (*layers*) that are applied sequentially to the input feature.\n",
    "The resulting features can be used for multiple tasks, such as classification using softmax regression.\n",
    "\n",
    "We will not cover the topic of deep learning here, we rather view neural nets as a blackbox that works as a feature map.\n",
    "\n",
    "## Pytorch\n",
    "\n",
    "We will use a pretrained deep neural network from the deep learning library [Pytorch](https://pytorch.org/).\n",
    "\n",
    "Install Pytorch and run the imports below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f53f87d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: make sure you can import this\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import datasets, models, transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e55605c",
   "metadata": {},
   "source": [
    "## ResNet18\n",
    "\n",
    "[ResNet18](https://arxiv.org/pdf/1512.03385.pdf) is a special case of a Deep Learning architecture, that was developed by Microsoft in 2015. It is called ResNet18, because it has 18 layers. \n",
    "\n",
    "We want to use ResNet18 as a feature map, that transforms our pixel feature vectors in some other representation. \n",
    "\n",
    "<div>\n",
    "<img src=\"dl_features.png\" width=\"300\"/>\n",
    "</div>\n",
    "\n",
    "Pytorch offers the opportunity to use several [pretrained models](https://pytorch.org/vision/0.8/models.html). \n",
    "The pretrained ResNet18 was trained to classify images from [ImageNet](http://www.image-net.org/). \n",
    "ImageNet is a image classification challenge, where RGB images have to be classified into one of 1000 classes.\n",
    "Therefore the output of ResNet18 is a vector of size 1000, with the entries showing the confidence in the respective class.\n",
    "\n",
    "The last layer in ResNet18 corresponds to $\\theta$ from the softmax regression. However the pretrained ResNet18 was trained on a different image classification task. Therefore this $\\theta$ is of no good for us. We are only interested in the features vectors that are fed into the softmax regression.\n",
    "\n",
    "An easy way to ignore the final softmax layer and to output the input to this layer is to replace it with a dummy layer, that does nothing but return its input. Below you find an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d42966b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Identity(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Identity, self).__init__()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return x\n",
    "    \n",
    "model = models.resnet18(pretrained=True)\n",
    "model.fc = Identity()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc8f3052",
   "metadata": {},
   "source": [
    "As mentioned before, ResNet18 was trained on a different dataset and therefore has special requirements to its inputs.\n",
    "\n",
    "ResNet18 expects its input to be\n",
    "- a torch tensor\n",
    "- of shape $b\\times 3\\times 224\\times 224$, with $b$ as the batch size (number of images that are processed at once)\n",
    "- normalized in a special way\n",
    "\n",
    "Pytorch provides all the functionality to preprocess our grayscale input images into the desired form. \n",
    "\n",
    "The transformation `preprocess` acts as a function that does all the necessary preprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c713d40",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess = transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Lambda(lambda x: torch.cat([x,x,x], axis=0)),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    transforms.Lambda(lambda x: x.unsqueeze(0))\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db818f46",
   "metadata": {},
   "source": [
    "### Task 10 (2 Points)\n",
    "\n",
    "Use the preprocess transformation on an image from the dataset. Preprocess is a function that takes an image (or a batch more on that below) as argument.  \n",
    "Feed this preprocessed image trough the ResNet18 model.\n",
    "Print the shapes of the preprocessed features and the ResNet18 features.\n",
    "\n",
    "Hints:\n",
    "- look [here](https://pytorch.org/vision/stable/auto_examples/plot_transforms.html#sphx-glr-auto-examples-plot-transforms-py) for examples of the transforms.\n",
    "- look [here](https://pytorch.org/tutorials/beginner/blitz/neural_networks_tutorial.html) for an example of a neural network usage in Pytorch (We will only need )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f26f1d04",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# TODO: use preprocess on image + print shape\n",
    "\n",
    "# TODO: feed preprocessed trough model + print shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d7ed61b",
   "metadata": {},
   "source": [
    "### Task 11 (5 Points)\n",
    "\n",
    "Now transform the train and testset into ResNet18 features. Implement a function, that converts a given dataset into ResNet18 features. Use this function on the train- and testset.\n",
    "\n",
    "**Important**: Run your transformation within the python [context](https://book.pythontips.com/en/latest/context_managers.html) `with torch.no_grad():` to reduce memory, since we dont need the gradients.\n",
    "\n",
    "Hints:\n",
    "- You can feed multiple images trough the model at once, if you [pack them together](https://pytorch.org/docs/stable/generated/torch.cat.html) in a batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75350c9c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def transform_dl(X, batchsize=50):\n",
    "    # TODO: convert images from X into ResNet18 features\n",
    "    pass\n",
    "\n",
    "# TODO: transform train- and testset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc4becf1",
   "metadata": {},
   "source": [
    "### Task 12 (2 Points)\n",
    "\n",
    "Use these features to perform logistic regression. What is the accuracy on the train- and testset now?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a4442d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: apply softmax regression on transformed train- and testset"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "f3375d8232a8291128f5034777637a4cf064a372b22289bd2848c4196289a98d"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 ('torch2')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
