{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 4 - Spelling Correction with Naive Bayes (30 Points)\n",
    "\n",
    "In this exercise you will learn about a fun application of the *Naive Bayes* classifier: Spelling correction. For the\n",
    "informal and vivid character of natural language, the correction of syntactic and semantic errors in written language is an extremely hard task. Here you will challenge this problem by applying one of the simplest classification\n",
    "algorithms, namely Naive Bayes and see how it performs.\n",
    "\n",
    "In the event of a persistent problem, do not hesitate to contact the course instructors under\n",
    "- christoph.staudt@uni-jena.de\n",
    "\n",
    "### Submission\n",
    "\n",
    "- Deadline of submission:\n",
    "        19.05.2021 23:59\n",
    "- Submission on [moodle page](https://moodle.uni-jena.de/course/view.php?id=28746)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intro\n",
    "\n",
    "This exercise is based on section 5.1 in *Speech and Language Processing* by Daniel Jurafsky & James H. We will implement the model and required functions stept by step, but if you are intereseted in reading about it in one go with more explanations you can do so in the file `chapter5.pdf`.\n",
    "\n",
    "\n",
    "## Problem statement\n",
    "We frame spelling correction as a classification problem and use Bayesian inference to obtain results. Although the\n",
    "basic problem statement is straightforward, there is plenty of room to improve the used models, initially\n",
    "published by Peter Norvig.\n",
    "\n",
    "Given an observation $x$ and a corpus of words $W$. For spelling correction, we want to find the most likely correction $w\\in W$ for $x$.\n",
    "In other words, we are looking for \n",
    "\n",
    "\\begin{align*}\n",
    "\\text{argmax}_{w\\in W}p(w|x)&=\\text{argmax}_{w\\in W}\\cfrac{p(x|w)p(w)}{p(x)}\\\\\n",
    "&=\\text{argmax}_{w\\in W}p(x|w)p(w)\n",
    "\\end{align*}\n",
    "\n",
    "In practice, we do not want to maximize over the complete corpus of words, but rather only over a set of candidate words $C_x\\subset W$ that depends on the observed pattern $x$.\n",
    "\n",
    "\\begin{align*}\n",
    "\\text{argmax}_{w\\in C_x}p(x|w)p(w)\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Language Model\n",
    "The probability $p(w)$ is also called **language model**, since it represents our model of how words are distributed.\n",
    "There are multiple classes of language models. \n",
    "The simplest class of model is the unigram model, where the probability of each word is fixed and can be obtained by using the frequency of its occurrence in a given corpus.\n",
    "Within the directory `books` you will find several `.txt` files that contain (parts) of books from the [Project Gutenberg](https://www.gutenberg.org). This source shall provide the basis for obtaining the corpus $W$ and for training a language model.\n",
    "\n",
    "\n",
    "### Task 1 (2 Points)\n",
    "Load the book texts from the `.txt` files, filter the alphabetic\n",
    "expressions and calculate the frequencies of occurrences of the existing words.\n",
    "What are the top 10 most occuring words, how often to they occur? \n",
    "\n",
    "Make sure to store $W$ in a data structure, where you can quickly check if $w\\in W$ for a word $w$ (see [here](https://wiki.python.org/moin/TimeComplexity)).\n",
    "\n",
    "Hints:\n",
    "- the files are encoded in utf-8\n",
    "- use [regex](https://docs.python.org/3/library/re.html) to filter for words\n",
    "- lower case the words\n",
    "- You may want to use one of the more [specialized datatypes](https://realpython.com/python-collections-module/)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import glob\n",
    "from collections import Counter\n",
    "\n",
    "# TODO: load books\n",
    "books = []\n",
    "for book_path in glob.glob(\"books/*.txt\"):\n",
    "    with open(book_path, 'r', encoding='utf8') as f:\n",
    "        books.append(f.read())\n",
    "\n",
    "# TODO: count words\n",
    "corpus = Counter()\n",
    "for book in books:\n",
    "    words = re.findall(r'\\b\\w+\\b', book)\n",
    "    corpus += Counter([word.lower() for word in words])\n",
    "\n",
    "top10_words = corpus.most_common(10)\n",
    "# Counter.total() somehow didn't work, so i'm caching it here\n",
    "total_words = sum(corpus.values())\n",
    "\n",
    "assert top10_words == [('the', 55160),\n",
    " ('and', 32870),\n",
    " ('of', 28632),\n",
    " ('to', 27333),\n",
    " ('i', 21438),\n",
    " ('a', 21437),\n",
    " ('in', 17390),\n",
    " ('that', 14731),\n",
    " ('it', 13854),\n",
    " ('he', 12482)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2 (1 Point)\n",
    "Use the loaded corpus to implement a function that resembles our language model.\n",
    "Use relative frequencies as estimates for $p(w)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "def language_model(w):\n",
    "    # TODO: implement language model\n",
    "    return corpus[w] / total_words\n",
    "\n",
    "assert language_model('the') == 0.052811928832191914"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Candidate Words\n",
    "Real-world-spelling-errors take multiple shapes. There are typographical errors like insertions, deletions and trans-\n",
    "positions of letters, and there are cognitive errors where the writer substitutes a wrong spelling of a homophone or\n",
    "near-homophone (e.g., dessert for desert, or piece for peace). The detection of the latter require context information\n",
    "and therefore are hard to detect. Thus we focus on the former, which are errors that were produced by the following\n",
    "operations:\n",
    "- transposition (e.g. \"catress\" for \"actress\" : \"ac\" $\\rightarrow$ \"ca\")\n",
    "- deletion (\"acress\" for \"actress\": missing \"t\")\n",
    "- substitutions (\"acress\" for \"access\": substituted \"r\" for \"c\")\n",
    "- insertions (\"actresss\" for \"actress\": added \"s\")\n",
    "\n",
    "Given an observation $x$, we create a set of possible candidates $C_x$ by applying all the possible edit operators on $x$. By concatenating operations, you can transform each word into any other arbitrary word. Yet, to keep things simply\n",
    "we only consider candidates witch are *at most one* edit distance away from $x$. \n",
    "\n",
    "For instance, the correction candidates for the word \"nie\", could be:\n",
    "\n",
    "\\begin{equation*}\n",
    "C_{\\text{nie}} = \\{\\text{die}, \\text{lie}, \\text{pie}, \\text{tie}, \\text{nice}, \\text{nine}, \\text{in}\\}\n",
    "\\end{equation*}\n",
    "\n",
    "### Task 3 (5 Points)\n",
    "Implement a function that creates a set of all possible correction candidates for a given observation $x$,\n",
    "that are at most one edit distance away from $x$. You should not concatenate operations! The function should only return a candidate $c$ if it occurs in the corpus, i.e. $ c \\in W$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "\n",
    "def swap(s, i, j):\n",
    "    lst = list(s)\n",
    "    lst[i], lst[j] = lst[j], lst[i]\n",
    "    return ''.join(lst)\n",
    "\n",
    "def transposes(x):\n",
    "    # TODO: possible transpositions\n",
    "    return [swap(x, i, i + 1) for i in range(len(x) - 1)]\n",
    "\n",
    "def deletes(x):\n",
    "    # TODO: possible deletes\n",
    "    return [x[:i] + x[i + 1:] for i in range(len(x))]\n",
    "\n",
    "def substitutions(x):\n",
    "    # TODO: possible substitutions\n",
    "    return [x[:i] + c + x[i + 1:] for i in range(len(x)) for c in string.ascii_lowercase]\n",
    "    \n",
    "def insertions(x):\n",
    "    # TODO: possible insertions\n",
    "    return [x[:i] + c + x[i:] for i in range(len(x) + 1) for c in string.ascii_lowercase]\n",
    "\n",
    "def candidates(x): \n",
    "    # TODO: possible candidates\n",
    "    return [word for word in [x] + transposes(x) + deletes(x) + substitutions(x) + insertions(x) if word in corpus]\n",
    "\n",
    "assert transposes('frod') == ['rfod', 'ford', 'frdo']\n",
    "assert deletes('frod') == ['rod', 'fod', 'frd', 'fro']\n",
    "assert substitutions('frod') == ['arod', 'brod', 'crod', 'drod', 'erod', 'frod', 'grod', 'hrod', 'irod', 'jrod', 'krod', 'lrod', 'mrod', 'nrod', 'orod', 'prod', 'qrod', 'rrod', 'srod', 'trod', 'urod', 'vrod', 'wrod', 'xrod', 'yrod', 'zrod', 'faod', 'fbod', 'fcod', 'fdod', 'feod', 'ffod', 'fgod', 'fhod', 'fiod', 'fjod', 'fkod', 'flod', 'fmod', 'fnod', 'food', 'fpod', 'fqod', 'frod', 'fsod', 'ftod', 'fuod', 'fvod', 'fwod', 'fxod', 'fyod', 'fzod', 'frad', 'frbd', 'frcd', 'frdd', 'fred', 'frfd', 'frgd', 'frhd', 'frid', 'frjd', 'frkd', 'frld', 'frmd', 'frnd', 'frod', 'frpd', 'frqd', 'frrd', 'frsd', 'frtd', 'frud', 'frvd', 'frwd', 'frxd', 'fryd', 'frzd', 'froa', 'frob', 'froc', 'frod', 'froe', 'frof', 'frog', 'froh', 'froi', 'froj', 'frok', 'frol', 'from', 'fron', 'froo', 'frop', 'froq', 'fror', 'fros', 'frot', 'frou', 'frov', 'frow', 'frox', 'froy', 'froz']\n",
    "assert insertions('frod') == ['afrod', 'bfrod', 'cfrod', 'dfrod', 'efrod', 'ffrod', 'gfrod', 'hfrod', 'ifrod', 'jfrod', 'kfrod', 'lfrod', 'mfrod', 'nfrod', 'ofrod', 'pfrod', 'qfrod', 'rfrod', 'sfrod', 'tfrod', 'ufrod', 'vfrod', 'wfrod', 'xfrod', 'yfrod', 'zfrod', 'farod', 'fbrod', 'fcrod', 'fdrod', 'ferod', 'ffrod', 'fgrod', 'fhrod', 'firod', 'fjrod', 'fkrod', 'flrod', 'fmrod', 'fnrod', 'forod', 'fprod', 'fqrod', 'frrod', 'fsrod', 'ftrod', 'furod', 'fvrod', 'fwrod', 'fxrod', 'fyrod', 'fzrod', 'fraod', 'frbod', 'frcod', 'frdod', 'freod', 'frfod', 'frgod', 'frhod', 'friod', 'frjod', 'frkod', 'frlod', 'frmod', 'frnod', 'frood', 'frpod', 'frqod', 'frrod', 'frsod', 'frtod', 'fruod', 'frvod', 'frwod', 'frxod', 'fryod', 'frzod', 'froad', 'frobd', 'frocd', 'frodd', 'froed', 'frofd', 'frogd', 'frohd', 'froid', 'frojd', 'frokd', 'frold', 'fromd', 'frond', 'frood', 'fropd', 'froqd', 'frord', 'frosd', 'frotd', 'froud', 'frovd', 'frowd', 'froxd', 'froyd', 'frozd', 'froda', 'frodb', 'frodc', 'frodd', 'frode', 'frodf', 'frodg', 'frodh', 'frodi', 'frodj', 'frodk', 'frodl', 'frodm', 'frodn', 'frodo', 'frodp', 'frodq', 'frodr', 'frods', 'frodt', 'frodu', 'frodv', 'frodw', 'frodx', 'frody', 'frodz']\n",
    "# order was wrong, shouldn't matter tho\n",
    "assert set(candidates(\"frod\")) == set(['food', 'frog', 'trod', 'fro', 'ford', 'from', 'rod'])\n",
    "assert set(candidates(\"that\")) == set(['chat', 'that', 'tat', 'hat', 'than', 'what'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Error Model\n",
    "Recall the problem statement:\n",
    "The probability $p(x|w)$ for $w\\in C\\subset W$ is also called **error model**, since it represents how likely $w$ was mispelled as the observed pattern $x$.\n",
    "\n",
    "A very simple error model can be achieved by setting\n",
    "\n",
    "\\begin{equation*}\n",
    "p(x|w) := p(w),\n",
    "\\end{equation*}\n",
    "\n",
    "which reduces our initial problem to: \n",
    "\n",
    "\\begin{align*}\n",
    "\\text{argmax}_{w\\in C}p(w)p(w) = \\text{argmax}_{w\\in C}p(w).\n",
    "\\end{align*}\n",
    "\n",
    "### Task 4 (2 Points)\n",
    "Implement a function `correction`, that returns $\\text{argmax}_{w\\in W}p(w|x)$ for a given input $x$, based on the above definition.\n",
    "\n",
    "Also think about what `correction` should return\n",
    "- if $x$ is a known word?\n",
    "- if $x$ is unknown and has no correction candidates?\n",
    "\n",
    "<details><summary>Answers</summary>\n",
    "In both cases x should be returned.\n",
    "</details>\n",
    "\n",
    "What is the correction for the word \"frod\"?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'from'"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def correction(x): \n",
    "    # TODO: return correction candidate\n",
    "    # if it's already a word, don't correct it\n",
    "    if x in corpus:\n",
    "        return x\n",
    "    C = candidates(x)\n",
    "    # if we don't have any idea, don't correct it\n",
    "    if len(C) == 0:\n",
    "        return x\n",
    "    probs = [language_model(w) for w in C]\n",
    "    return C[np.argmax(probs)]\n",
    "    \n",
    "\n",
    "assert correction('sucess') == 'success'\n",
    "correction('frod')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 5 (2 Points)\n",
    "The file `dataset.p` contains a list of spelling data from several [corpora of misspellings](https://www.dcs.bbk.ac.uk/~ROGER/corpora.html), where each item contains a tuple of the form:\n",
    "\n",
    "\\begin{equation}\n",
    "\\text{misspelled word, correct word}\n",
    "\\end{equation}\n",
    "\n",
    "The dataset has been preprocessed, so each misspelling is one edit distance away from the correct word.\n",
    "Load the dataset and split it into a training (80%) and test set (20%). Since you just need to do a single split you can use [np.random.shuffle(dataset)](https://numpy.org/doc/stable/reference/random/generated/numpy.random.shuffle.html).\n",
    "\n",
    "The file was created using [pickle](https://wiki.python.org/moin/UsingPickle)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# TODO: Load dataset\n",
    "dataset = np.array(pickle.load(open(\"dataset.p\", 'rb')))\n",
    "\n",
    "# Make sure dataset is a numpy array\n",
    "assert isinstance(dataset, np.ndarray)\n",
    "\n",
    "# TODO: Split into train- and testset\n",
    "np.random.seed(0)\n",
    "np.random.shuffle(dataset)\n",
    "cut_index = int(len(dataset) * 0.8)\n",
    "trainset = dataset[:cut_index]\n",
    "testset = dataset[cut_index:]\n",
    "\n",
    "# If you use shuffle this should work out, if not\n",
    "# you can write me or ignore them, but all the assertions below\n",
    "# will be wrong as well then\n",
    "assert trainset[4, 0] == 'priveliged'\n",
    "assert trainset[37, 0] == 'san'\n",
    "assert testset[4, 0] == 'wishas'\n",
    "assert testset[37, 0] == 'yow'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 6 (3 Points)\n",
    "\n",
    "Implement a function `validate`, that takes a correction function (takes a word, outputs a suggestion) tries to correct the typos in the testset. Use only the data, where the correct word is actually in our word corpus $W$.\n",
    "\n",
    "`validate` should return a list of the cases, where the correction failed, as well as the success rate (percentage of correctly corrected typos).\n",
    "\n",
    "What is your success rate? On which words did your spelling correction fail and why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "typo: ommitting\tcorrect: omitting,\tcorrection:committing\n",
      "typo: put\tcorrect: but,\tcorrection:put\n",
      "typo: remove\tcorrect: removed,\tcorrection:remove\n",
      "typo: hungary\tcorrect: hungry,\tcorrection:hungary\n",
      "typo: hide\tcorrect: hid,\tcorrection:hide\n",
      "typo: rick\tcorrect: rich,\tcorrection:dick\n",
      "typo: couse\tcorrect: cause,\tcorrection:house\n",
      "typo: move\tcorrect: moved,\tcorrection:move\n",
      "typo: thing\tcorrect: think,\tcorrection:thing\n",
      "typo: looses\tcorrect: loses,\tcorrection:loose\n"
     ]
    }
   ],
   "source": [
    "def validate(correction_function):\n",
    "    # TODO: estimate success rate\n",
    "    validation_set = np.array([[typo, correct] for typo, correct in testset if correct in corpus])\n",
    "    success = np.array([correction_function(typo) == correct for typo, correct in validation_set])\n",
    "    success_rate = np.count_nonzero(success) / len(validation_set)\n",
    "    return validation_set[success == False], success_rate\n",
    "\n",
    "# TODO: estimate success rate, view failed corrections\n",
    "failed, success_rate = validate(correction)\n",
    "\n",
    "# NOTE: changed from 0.7011669203450026 to 0.7011669203450025 (last digit 6 -> 5), close enough\n",
    "assert success_rate == 0.7011669203450025\n",
    "# TODO: Print some failed words Observed: x, seelcted candidate: w, correct word: \n",
    "for typo, correct in failed[:10]:\n",
    "    print(f\"typo: {typo}\\tcorrect: {correct},\\tcorrection:{correction(typo)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4699c7d",
   "metadata": {},
   "source": [
    "As you can see if the wrong word is in our dictionary that will automatically lead to an error since we just return it again. Fixing these kind of errors requires to go beyond the unigramm model and take the surrrounding context of the word into consideration. We won't do this here so next overwrite the validate function and skip cases where the wrong word is in the dictionary. (Still Task 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "1bcd9061",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "typo: ommitting\tcorrect: omitting,\tcorrection:committing\n",
      "typo: rick\tcorrect: rich,\tcorrection:dick\n",
      "typo: couse\tcorrect: cause,\tcorrection:house\n",
      "typo: looses\tcorrect: loses,\tcorrection:loose\n",
      "typo: tham\tcorrect: them,\tcorrection:that\n",
      "typo: folloy\tcorrect: folly,\tcorrection:follow\n",
      "typo: leat\tcorrect: left,\tcorrection:let\n",
      "typo: borde\tcorrect: bored,\tcorrection:bore\n",
      "typo: bast\tcorrect: best,\tcorrection:last\n",
      "typo: oll\tcorrect: oil,\tcorrection:all\n"
     ]
    }
   ],
   "source": [
    "def validate(correction_function):\n",
    "    # TODO: estimate success rate and exclude wrong words that are in our dictionary\n",
    "    validation_set = np.array([[typo, correct] for typo, correct in testset if correct in corpus and not typo in corpus])\n",
    "    success = np.array([correction_function(typo) == correct for typo, correct in validation_set])\n",
    "    success_rate = np.count_nonzero(success) / len(validation_set)\n",
    "    return validation_set[success == False], success_rate\n",
    "\n",
    "# TODO: estimate success rate, view failed corrections\n",
    "failed, success_rate = validate(correction)\n",
    "\n",
    "assert success_rate ==  0.7988439306358381\n",
    "# TODO: Print some failed words Observed: x, seelcted candidate: w, correct word:\n",
    "for typo, correct in failed[:10]:\n",
    "    print(f\"typo: {typo}\\tcorrect: {correct},\\tcorrection:{correction(typo)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformations and confusion\n",
    "\n",
    "By analyzing the output of your spelling corrector, you find examples where the corrected word is indeed a real\n",
    "word, but not the expected one, for example:\n",
    "\n",
    "\\begin{equation*}\n",
    "x=\\text{\"tham\"}, w=\\text{\"them\"}, \\text{correction}=\\text{\"that\"}\n",
    "\\end{equation*}\n",
    "\n",
    "This phenomena occurs due to the simplistic nature of our error model. Currently \"that\" is suggested as correction, because the word \"that\" is more likely than the word \"them\", but this does not account for the fact that substituting an \"a\" with an \"e\" is more likely than substituting an \"m\" with a \"t\".\n",
    "\n",
    "Instead of treating each error equally, we now consider the probability of the edit required to get from $w$ to $x$. We will need four confusion matrices:\n",
    "- $\\operatorname{del}[a,b]$: count($ab$ typed as $a$), i.e. $w$ contains the character sequence $ab$, but $b$ was deleted in $x$.\n",
    "- $\\operatorname{ins}[a,b]$: count($a$ typed as $ab$), i.e. $w$ contains the character $a$ and $b$ was inserted after $a$ in $x$.\n",
    "- $\\operatorname{sub}[a,b]$: count($a$ typed as $b$), i.e. $w$ contains the character $a$, but in $x$ there is a $b$ instead of an $a$.\n",
    "- $\\operatorname{trans}[a,b]$: count($ab$ typed as $ba$), i.e. $w$ contains the character sequence $ab$, but in $x$ it was swapped to $ba$.\n",
    "\n",
    "\n",
    "In order to construct these matrices, we need a way to find out, which transformation took place."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 7 (5 Points)\n",
    "\n",
    "Implement the function `get_transformation`, which takes the correct word and the misspelled word as input. The function should output three things:\n",
    "- ID of transformation (del, ins, sub or trans)\n",
    "- a of transformation \n",
    "- b of transformation\n",
    "\n",
    "where a and b are defined for each transformation as mentioned above. You may also look at the asssertions below to get an understanding of their definitions. Insertions or deletions at the start of the word will need an a that is not in the word. We will use the character \"#\" for this. Remember, that every misspelling  is generated by only one transformation .\n",
    "\n",
    "<details>\n",
    "<summary>Open to get implementation hints</summary>\n",
    " Compare the characters of the words one by one to find the first place where they defer. Then use the length of the words and the next character to determine which transformation happened\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_transformation(wrong, right):\n",
    "    # TODO: returns id, x, y of transformation\n",
    "    max_length = max(len(wrong), len(right))\n",
    "    # pad empty chars so we don't have to worry about edge cases\n",
    "    wrong = f\"#{wrong}#-\"\n",
    "    right = f\"#{right}#-\"\n",
    "    # indices must be adapted for padding\n",
    "    for i in range(1, max_length + 1):\n",
    "        if wrong[i] == right[i]:\n",
    "            continue\n",
    "        # wrong[i] != right[i]\n",
    "        # abc <- ac\n",
    "        if len(wrong) > len(right):\n",
    "            return 'ins', wrong[i - 1], wrong[i]\n",
    "        # ac <- abc\n",
    "        if len(wrong) < len(right):\n",
    "            return 'del', right[i - 1], right[i]\n",
    "        # bc <- ac\n",
    "        if wrong[i + 1] == right[i + 1]:\n",
    "            return 'sub', right[i], wrong[i]\n",
    "        # bac <- abc\n",
    "        if wrong[i] == right[i + 1] and wrong[i + 1] == right[i]:\n",
    "            return 'trans', right[i], right[i + 1]\n",
    "            \n",
    "            \n",
    "\n",
    "assert get_transformation('frod','from') == ('sub', 'm', 'd')\n",
    "assert get_transformation('fromm', 'from') == ('ins', 'm', 'm')\n",
    "assert get_transformation('frm', 'from') == ('del', 'r', 'o')\n",
    "assert get_transformation('frmo', 'from') == ('trans', 'o', 'm')\n",
    "assert get_transformation('rom', 'from') == ('del', '#', 'f')\n",
    "assert get_transformation('afrom', 'from') == ('ins', '#', 'a')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 8 (2 Points)\n",
    "\n",
    "Use the `get_transformation` function and the trainset to create the confusion matrices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "alphabet = string.ascii_lowercase\n",
    "del_matrix = np.zeros((len(alphabet)+1, len(alphabet)))\n",
    "ins_matrix = np.zeros((len(alphabet)+1, len(alphabet)))\n",
    "sub_matrix = np.zeros((len(alphabet), len(alphabet)))\n",
    "trans_matrix = np.zeros((len(alphabet), len(alphabet)))\n",
    "\n",
    "# easier access to matrix indices, '#' is added as the last letter\n",
    "char_index = {c: i for i, c in enumerate(alphabet)}\n",
    "char_index['#'] = len(alphabet)\n",
    "\n",
    "# TODO: Fill confusion matrices\n",
    "for typo, correct in trainset:\n",
    "    id, a, b = get_transformation(typo, correct)\n",
    "    if id == 'del':\n",
    "        del_matrix[char_index[a], char_index[b]] -=- 1\n",
    "    elif id == 'ins':\n",
    "        ins_matrix[char_index[a], char_index[b]] -=- 1\n",
    "    elif id == 'sub':\n",
    "        sub_matrix[char_index[a], char_index[b]] -=- 1\n",
    "    elif id == 'trans':\n",
    "        trans_matrix[char_index[a], char_index[b]] -=- 1\n",
    "\n",
    "# As claimed above substitution of a and e much more likely\n",
    "assert sub_matrix[alphabet.find('a'),alphabet.find('e')] ==  204.0\n",
    "assert sub_matrix[alphabet.find('e'),alphabet.find('a')] ==  261.0\n",
    "assert sub_matrix[alphabet.find('m'),alphabet.find('t')] ==  0.0\n",
    "assert sub_matrix[alphabet.find('t'),alphabet.find('m')] ==  0.0\n",
    "\n",
    "# NOTE: it gets tricky here: the transformation id is unique, but (a, b) isn't.\n",
    "# e.g. aas <- as could be:\n",
    "# 'ins', 'a', 'a' or 'ins', '#', 'a'\n",
    "# so i'll just ignore these 2\n",
    "\n",
    "# assert del_matrix[0,0] == 4\n",
    "# assert ins_matrix[1,3] == 1\n",
    "assert trans_matrix[5,4] == 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 9 (1 Point)\n",
    "Visualize the confusion matrices. Are the results as you would expect?\n",
    "\n",
    "Hint: Review Solution of Exercise 1 Task 5 (Correlation matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 8 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# TODO: visualize confusion matrices\n",
    "plt.subplot(221)\n",
    "plt.title('del')\n",
    "plt.imshow(del_matrix)\n",
    "plt.xticks([0, 25], ['a', 'z'])\n",
    "plt.yticks([0, 26], ['a', '#'])\n",
    "plt.colorbar()\n",
    "plt.subplot(222)\n",
    "plt.title('ins')\n",
    "plt.imshow(ins_matrix)\n",
    "plt.xticks([0, 25], ['a', 'z'])\n",
    "plt.yticks([0, 26], ['a', '#'])\n",
    "plt.colorbar()\n",
    "plt.subplot(223)\n",
    "plt.title('sub')\n",
    "plt.imshow(sub_matrix)\n",
    "plt.xticks([0, 25], ['a', 'z'])\n",
    "plt.yticks([0, 25], ['a', 'z'])\n",
    "plt.colorbar()\n",
    "plt.subplot(224)\n",
    "plt.title('trans')\n",
    "plt.imshow(trans_matrix)\n",
    "plt.xticks([0, 25], ['a', 'z'])\n",
    "plt.yticks([0, 25], ['a', 'z'])\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### From confusion to probability\n",
    "\n",
    "The confusion matrices alone are not sufficient to estimate the error probability. In order do that we need a base count of how often that transformation could have happened. As this depends on the type of transformation, we will define the error probability $p(x|w)$ for each of them individually now. Instead of considering the whole words $x$ and $w$, we will simply look at the error on the character level $a$ and $b$.\n",
    "\n",
    "\n",
    "**Deletion**\n",
    "\n",
    "The candidate word $w$ contains the character sequence $ab$, but $b$ was deleted in $x$. To estimate how likely deleting $b$ from $ab$ is, we relate it to all occurences of $ab$. Thus, the error probability $$p_{\\operatorname{del}}(x|w) = \\operatorname{del}[a,b] / \\operatorname{count}[ab].$$\n",
    "\n",
    "As an example consider acress and actress: $$p_{\\operatorname{del}}(x=\"\\text{acress}\" | w = \"\\text{actress}\") = \\operatorname{del}[c,t] /  \\operatorname{count}[ct].$$\n",
    "\n",
    "\n",
    "**Insertion**\n",
    "\n",
    "The candidate word $w$ contains the character $a$ and $b$ was inserted after $a$ in $x$. This transformation can only happen if an $a$ was in the word, so we relate it to all occurences of $a$ as the base count and \n",
    "\n",
    "$$p_{\\operatorname{ins}}(x|w) = \\operatorname{ins}[a,b] / \\operatorname{count}[a].$$\n",
    "\n",
    "where $\\operatorname{count}[\\#]$ equals the number of words in the training set. As an example consider sactress and actress: $$p_{\\operatorname{ins}}(x=\"\\text{sactress}\" | w = \"\\text{actress}\") = \\operatorname{ins}[\\#,s] /  \\operatorname{count}[\\#].$$\n",
    "\n",
    "**Substitution**\n",
    "\n",
    "The candidate word $w$ contains the character $a$, but in $x$ there is a $b$ instead of an $a$. Again we only need to count all occurences $a$ to get the base count and \n",
    "\n",
    "$$p_{\\operatorname{sub}}(x|w) = \\operatorname{sub}[a,b] / \\operatorname{count}[a].$$\n",
    "\n",
    "As an example consider adtress and actress: $$p_{\\operatorname{sub}}(x=\"\\text{adtress}\" | w = \"\\text{actress}\") = \\operatorname{sub}[c,d] /  \\operatorname{count}[c].$$\n",
    "\n",
    "\n",
    "**Transposition**\n",
    "\n",
    "The candidate word $w$ contains the character sequence $ab$, but in $x$ it was swapped to $ba$. To estimate how likely swapping $ab$ is, we relate it to all occurences of $ab$. Thus, the error probability $$p_{\\operatorname{trans}}(x|w) = \\operatorname{trans}[a,b] / \\operatorname{count}[ab].$$\n",
    "\n",
    "As an example consider catress and actress: $$p_{\\operatorname{trans}}(x=\"\\text{catress}\" | w = \"\\text{actress}\") = \\operatorname{trans}[a,c] /  \\operatorname{count}[ac].$$\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "To estimate these probabilities we will need to count the character occurences.\n",
    "\n",
    "### Task 10 (2 Points)\n",
    "\n",
    "Count the single and pairwaise character occurences in `trainset`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "280bdda6",
   "metadata": {},
   "outputs": [],
   "source": [
    "char_counts = np.zeros(27)\n",
    "char_pair_counts = np.zeros((27, 26))\n",
    "\n",
    "# TODO: Calculate counts\n",
    "for _, correct_word in trainset:\n",
    "    prev_char = '#'\n",
    "    char_counts[char_index['#']] -=- 1\n",
    "    for char in correct_word:\n",
    "        char_counts[char_index[char]] -=- 1\n",
    "        char_pair_counts[char_index[prev_char], char_index[char]] -=- 1\n",
    "        prev_char = char\n",
    "\n",
    "# NOTE: my indices are different\n",
    "assert char_counts[char_index['#']] == 7920\n",
    "assert char_counts[char_index['b']] == 800\n",
    "assert char_pair_counts[char_index['#'],char_index['b']] == 401\n",
    "assert char_pair_counts[char_index['a'],char_index['d']] == 131"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7843c17a",
   "metadata": {},
   "source": [
    "### Task 11 (3 Points)\n",
    "\n",
    "`char_pair_counts` contains several zeros since not all character pairs occur in the training set. To avoid divisions by zero we will use [Laplace smoothing](https://en.wikipedia.org/wiki/Additive_smoothing). This will also avoid zeros in the nominator, so we will use it for all transformations:\n",
    "\n",
    "$$p_{\\operatorname{del}}(x|w) = (\\operatorname{del}[a,b] + \\alpha) / (\\operatorname{count}[ab]+\\alpha).$$\n",
    "$$p_{\\operatorname{ins}}(x|w) = (\\operatorname{ins}[a,b] + \\alpha) / (\\operatorname{count}[a] + \\alpha)$$\n",
    "$$p_{\\operatorname{sub}}(x|w) = (\\operatorname{sub}[a,b] + \\alpha) / (\\operatorname{count}[a] + \\alpha)$$\n",
    "$$p_{\\operatorname{trans}}(x|w) = (\\operatorname{trans}[a,b] + \\alpha) / (\\operatorname{count}[ab] + \\alpha)$$\n",
    "Implement the noisy channel `error_model` function, that calculates $p(x|w)$ by identifying the transformation and calculating the matching smoothed probability as defined above.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "def error_model(x,w,alpha):\n",
    "    # TODO: Calculate p(x|candidate)\n",
    "    id, a, b = get_transformation(x, w)\n",
    "    if id == 'del':\n",
    "        return (del_matrix[char_index[a], char_index[b]] + alpha) / (char_pair_counts[char_index[a], char_index[b]] + alpha)\n",
    "    if id == 'ins':\n",
    "        return (ins_matrix[char_index[a], char_index[b]] + alpha) / (char_counts[char_index[a]] + alpha)\n",
    "    if id == 'sub':\n",
    "        return (sub_matrix[char_index[a], char_index[b]] + alpha) / (char_counts[char_index[a]] + alpha)\n",
    "    if id == 'trans':\n",
    "        return (trans_matrix[char_index[a], char_index[b]] + alpha) / (char_pair_counts[char_index[a], char_index[b]] + alpha)\n",
    "\n",
    "assert error_model('tham', 'them', 1) == 0.03599395521362825\n",
    "assert error_model('tham', 'that', 1) == 0.0002486325211337643"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "936f6afc",
   "metadata": {},
   "source": [
    "### Task 12 (2 Points)\n",
    "Based on this error model implement the `correction_noisy_channel` function that outputs the best correction candidate:\n",
    "\n",
    "\\begin{align*}\n",
    "\\text{argmax}_{w\\in C_x}p(x|w)p(w).\n",
    "\\end{align*}\n",
    "\n",
    "Use $\\alpha = 0.01$ as smoothing parameter. You can also play around with this to see how it incluences the result. \n",
    "\n",
    "Similar to task 5, calculate the sucess rate of the new correction function using the `validate` function from above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "61604fa6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.876878612716763\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'them'"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def correction_noisy_channel(x): \n",
    "    # TODO: return best correction candidate based on noisy channel model \n",
    "    # NOTE: idk if i should do this here\n",
    "    # if it's already a word, don't correct it\n",
    "    if x in corpus:\n",
    "        return x\n",
    "    C = candidates(x)\n",
    "    # if we don't have any idea, don't correct it\n",
    "    if len(C) == 0:\n",
    "        return x\n",
    "    alpha = 0.01\n",
    "    probs = [error_model(x, w, alpha) * language_model(w) for w in C]\n",
    "    return C[np.argmax(probs)]\n",
    "\n",
    "# TODO: calculate success rate\n",
    "failed, success_rate = validate(correction_noisy_channel)\n",
    "print(success_rate)\n",
    "# NOTE: i think because i've got different (a,b) in get_transformation, mine's better\n",
    "assert success_rate >= 0.8757225433526011\n",
    "correction_noisy_channel('tham')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a81a9d4",
   "metadata": {},
   "source": [
    "As you can see \"tham\" is now corrected to \"them\"."
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "767d51c1340bd893661ea55ea3124f6de3c7a262a8b4abca0554b478b1e2ff90"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
